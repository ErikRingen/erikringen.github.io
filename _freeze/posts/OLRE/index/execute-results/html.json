{
  "hash": "c512387a67a9f28e68f2369aca48f772",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The right way to do predictive checks with observation-level random effects\"\nauthor: \"Erik J. Ringen\"\ndate: \"2024-11-10\"\ncategories: [Prediction, Statistics, brms, PyMC]\nimage: \"image.png\"\ncode-fold: true\ntoc: true\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\nObservation-level random effects (OLRE) are an effective way to model overdispersed count data [@harrison2014using]. For example, OLREs relax the assumption of Poisson regression that the variance is equal to the mean by adding a \"random intercept\" ($\\nu$) for every observation. For each observation $i$:\n\n$$ y_i \\sim \\text{Poisson}(\\lambda_i)$$\n$$ \\text{log}(\\lambda_i) = b_0 + \\nu_i$$\n$$ \\nu_i \\sim \\mathcal{N} (0, \\sigma) $$\n\nBy putting $\\nu$ inside the linear model, we smuggle a variance component ($\\sigma$) into a distribution that otherwise has only a single rate parameter (this trick also works for the Binomial distribution [@harrison2015comparison]). OLREs are also useful for [multi-response models](https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html), allowing us to capture residual correlations between multiple outcomes [@hadfield2010mcmc].\n\n[Predictive checks](https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html) are a routine form of model checking used to understand a model's ability to represent the data. Unfortunately, **most software for performing predictive checks will handle OLREs the wrong way by default, giving an inflated goodness-of-fit.** In this post I will show you how to do it the right way, in either R + `brms` or Python + `PyMC`. In this post I focus on Bayesian models, but the basic idea would also hold for checking frequentist models with software such as `lme4` and `glmmTMB`.\n\n# The wrong way\n\nIn Bayesian workflow, we often perform [posterior predictive checks](https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html). In brief, draws from the posterior distribution are used to generate many synthetic replications of our dataset, denoted $y_{\\text{rep}}$, which are then compared to the observed values. Systematic discrepancies between the distribution of $y_\\text{rep}$ and the actual data indicate misspecification, and can suggest ways to improve our model.\n\nTo illustrate this idea, we'll use the Oceanic toolkit complexity dataset from Michelle Kline and Robert Boyd [@kline2010population]. The response variable is the count of unique tools in a given Oceanic society (`total_tools`), which is predicted by the natural logarithm of population size (`population`). First, we will fit this basic model, using priors from [@mcelreath2020god]:\n\n$$ \\text{total\\_tools}_i \\sim \\text{Poisson}(\\lambda_i)$$\n$$ \\text{log}(\\lambda_i) = b_0 + b_{\\text{pop}}\\text{log}(\\text{population}_{z_i})$$\n$$ b_0 \\sim \\mathcal{N}(3, 0.5)$$\n$$ b_{\\text{pop}} \\sim \\mathcal{N}(0, 0.2)$$\n\nThen, we will perform posterior predictive checks using some off-the-shelf convenience functions.\n\n:::{.panel-tabset}\n\n## brms\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(patchwork)\nset.seed(123)\n\nKline <- read.csv(\"https://raw.githubusercontent.com/rmcelreath/rethinking/refs/heads/master/data/Kline.csv\", sep=\";\")\n\nKline$log_pop_z <- scale(log(Kline$population)) # standardize\n\nm_poisson <- brm(\n    total_tools ~ 1 + log_pop_z,\n    family = poisson(link = \"log\"),\n    prior = prior(normal(3, 0.5), class = \"Intercept\") + \n        prior(normal(0, 0.2), class = \"b\"),\n    chains = 1,\n    data = Kline,\n    seed = 123)\n\ncolor_scheme_set(\"teal\")\ntheme_set(theme_classic(base_size = 13))\n\nbrms::pp_check(m_poisson, type = \"dens_overlay\", ndraws = 200) + \n    theme(legend.position = \"none\") + \n    brms::pp_check(m_poisson, type = \"intervals\") + \n    plot_layout(guides = 'collect') + \n    theme_classic(base_size = 13) +\n    plot_annotation(subtitle = \"Basic Poisson PPC\") \n```\n\n::: {.cell-output-display}\n![Posterior predictive checks for basic Poisson model. (left) replicated and observed densities, (right) observation-level reps, with bars representing 50% and 90% credible intervals.](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n## PyMC\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.colors as cols\nimport matplotlib.pyplot as plt\nplt.rcParams.update({\n    'font.size': 30,\n    'axes.titlesize': 32,         \n    'axes.labelsize': 30,        \n    'xtick.labelsize': 28,      \n    'ytick.labelsize': 28, \n    'legend.fontsize': 32,\n    'lines.linewidth': 1.5,\n})\n\nimport pandas as pd\nimport pymc as pm\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import gaussian_kde\nimport arviz as az\n\nKline = pd.read_csv(\"https://raw.githubusercontent.com/rmcelreath/rethinking/refs/heads/master/data/Kline.csv\", sep=\";\")\n\nKline['log_pop_z'] = stats.zscore(np.log(Kline['population']))\n\nwith pm.Model() as m_poisson:\n    # priors\n    b0 = pm.Normal(\"Intercept\", mu=3, sigma=0.5)\n    b_pop = pm.Normal(\"slope\", mu=0, sigma=0.2)\n    # linear model\n    log_lam = b0 + b_pop * Kline['log_pop_z']\n    ## Poisson likelihood\n    y = pm.Poisson(\"y\", mu=pm.math.exp(log_lam), observed=Kline['total_tools'])\n\n    idata = pm.sample(4000, chains=1, random_seed=123)\n    pm.sample_posterior_predictive(idata, extend_inferencedata=True, random_seed=1);\n```\n\n```{.python .cell-code}\n\n\nidata.observed_data['y'] = idata.observed_data['y'].astype(np.float64) # Convert observed data to float for visualization\nidata.posterior_predictive['y'] = idata.posterior_predictive['y'].astype(np.float64)\n\n# Define plot functions\ndef plot_ppc_dens(y, yrep, ax, ax_index, num_samples=200):\n    yrep = yrep.values\n    y = y.values \n\n    for i in range(num_samples):\n        sample = yrep[0, i, :]\n        kde_sample = gaussian_kde(sample)\n        x_values = np.linspace(yrep.min(), yrep.max(), 200)\n        ax[ax_index].plot(x_values, kde_sample(x_values), color=(0.0, 0.486, 0.486, 0.05))  # Use low alpha for transparency\n\n    kde = gaussian_kde(y)\n    x_values = np.linspace(yrep.min(), yrep.max(), 200)\n    ax[ax_index].plot(x_values, kde(x_values), color=\"#007c7c\", linewidth=6)\n\n    ax[ax_index].set_xlabel('')\n    ax[ax_index].set_ylabel('')\n    ax[ax_index].margins(y=0)\n    ax[ax_index].margins(x=0)\n    ax[ax_index].spines['top'].set_visible(False)\n    ax[ax_index].spines['right'].set_visible(False)\n\ndef plot_ppc_intervals(y, yrep, ax, ax_index):\n    y = y.values\n    yrep = yrep.stack(sample=(\"chain\", \"draw\")).values\n\n    median_predictions = np.median(yrep, axis=1)\n\n    # Define x-axis values\n    x = np.arange(len(y))\n\n    intervals = [(25, 75), (5, 95)]\n    colors = ['#007C7C', '#007C7C']\n    labels = ['50% Interval', '90% Interval']\n\n    for (low, high), color, label in zip(intervals, colors, labels):\n        lower_bounds = np.percentile(yrep, low, axis=1)\n        upper_bounds = np.percentile(yrep, high, axis=1)\n        error_lower = median_predictions - lower_bounds\n        error_upper = upper_bounds - median_predictions\n        error = [error_lower, error_upper]\n\n        ax[ax_index].errorbar(\n        x,\n        median_predictions,\n        yerr=error,\n        fmt='o',\n        color='#007C7C',\n        ecolor=color,\n        elinewidth=8,\n        capsize=0,\n        label=label,\n        alpha = 0.10,\n        markersize=20\n    )\n\n    # Overlay observed data points\n    ax[ax_index].scatter(\n        x,\n        y,\n        color='#007C7C',\n        label='Observed Data',\n        zorder=5,\n        s = 150\n    )\n\n    # Customize the plot\n    ax[ax_index].set_xlabel('Data point (index)')\n    ax[ax_index].set_ylabel('')\n    ax[ax_index].set_title('')\n    ax[ax_index].legend(['y', 'yrep'], loc=\"upper left\")\n    ax[ax_index].spines['top'].set_visible(False)\n    ax[ax_index].spines['right'].set_visible(False)\n\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 13))\n\nplot_ppc_dens(idata.observed_data['y'], idata.posterior_predictive['y'], axes, 0)\nplot_ppc_intervals(idata.observed_data['y'], idata.posterior_predictive['y'], axes, 1)\naxes[0].set_title('Basic Poisson PPC', loc = \"left\")\n```\n\n::: {.cell-output-display}\n![Posterior predictive checks for basic Poisson model. (left) replicated and observed densities, (right) observation-level reps, with bars representing 50% and 90% credible intervals.](index_files/figure-html/unnamed-chunk-3-1.png){width=1920}\n:::\n:::\n\n\n\n:::\n\nWe see some indications that the data are overdispersed, relative to the model. Namely, the distribution of the observed $y$ (`total_tools`) appears \"flat\" compared to $y_{\\text{rep}}$ in the left-side plot. We can also see in the right-side plot that several observed values fall outside of the 90% credible intervals, suggesting that the model's predictions are too precise. So, lets try adding an OLRE to capture this overdispersion. Here's our updated model definition:\n\n$$ \\text{total\\_tools}_i \\sim \\text{Poisson}(\\lambda_i)$$\n$$ \\text{log}(\\lambda_i) = b_0 + \\nu_i + b_{\\text{pop}}\\text{log}(\\text{population}_{z_i})$$\n$$ b_0 \\sim \\mathcal{N}(3, 0.5)$$\n$$ b_{\\text{pop}} \\sim \\mathcal{N}(0, 0.2)$$\n$$ \\nu_i \\sim \\mathcal{N}(0, \\sigma)$$\n$$ \\sigma \\sim \\text{Exponential}(2) $$\n\n:::{.panel-tabset}\n\n## brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKline$obs <- 1:nrow(Kline)\n\nm_poisson_OLRE <- brm(\n    total_tools ~ 1 + log_pop_z + (1|obs),\n    family = poisson(link = \"log\"),\n    prior = prior(normal(3, 0.5), class = \"Intercept\") + \n        prior(normal(0, 0.2), class = \"b\") + \n        prior(exponential(2), class = \"sd\"),\n    chains = 1,\n    control = list(adapt_delta = 0.95),\n    data = Kline,\n    seed=123,\n    save_pars = save_pars(all = TRUE))\n\nbrms::pp_check(m_poisson_OLRE, type = \"dens_overlay\", ndraws = 200) + theme(legend.position = \"none\") + brms::pp_check(m_poisson_OLRE, type = \"intervals\") +\n    plot_annotation(subtitle = \"OLRE PPC: The wrong way\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n:::\n\n\n\n## PyMC\n\n\n::: {.cell}\n\n```{.python .cell-code}\nobs_idx = np.arange(len(Kline))\ncoords = {\"obs\": obs_idx}\n\nwith pm.Model(coords = coords) as m_poisson_OLRE:\n    # priors\n    b0 = pm.Normal(\"Intercept\", mu=3, sigma=0.5)\n    b_pop = pm.Normal(\"slope\", mu=0, sigma=0.2)\n    nu = pm.Normal(\"nu\", mu = 0, sigma = 1, dims = \"obs\")\n    sigma = pm.Exponential(\"sigma\", lam = 2)\n    # linear model\n    log_lam = b0 + nu[obs_idx]*sigma + b_pop * Kline['log_pop_z'] \n    ## Poisson likelihood\n    y = pm.Poisson(\"y\", mu=pm.math.exp(log_lam), observed=Kline['total_tools'])\n\n    idata_OLRE = pm.sample(4000, chains = 1, target_accept = 0.95, random_seed=123)\n    pm.sample_posterior_predictive(idata_OLRE, extend_inferencedata=True, random_seed=1);\n```\n\n```{.python .cell-code}\n\nidata_OLRE.observed_data['y'] = idata_OLRE.observed_data['y'].astype(np.float64) # Convert observed data to float for visualization\nidata_OLRE.posterior_predictive['y'] = idata_OLRE.posterior_predictive['y'].astype(np.float64)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 13))\n\nplot_ppc_dens(idata_OLRE.observed_data['y'], idata_OLRE.posterior_predictive['y'], axes, 0)\nplot_ppc_intervals(idata_OLRE.observed_data['y'], idata_OLRE.posterior_predictive['y'], axes, 1)\naxes[0].set_title('OLRE PPC: The wrong way', loc = \"left\")\n```\n\n::: {.cell-output-display}\n![Incorrect posterior predictive checks for OLRE Poisson model, using off-the-shelf convenience functions. (left) replicated and observed densities, (right) observation-level reps, with bars representing 50% and 90% credible intervals.](index_files/figure-html/unnamed-chunk-5-1.png){width=1920}\n:::\n:::\n\n\n\n:::\n\n:::{.panel-tabset}\n\n## brms\n\nLooks good, right? Sadly, this is a little too good to be true. We have mislead ourselves, and to see why, have a look at the $\\nu_{\\text{obs}}$ parameters in comparison to the observed data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_poisson_OLRE |> \n    spread_draws(r_obs[obs]) |> \n    median_qi(estimate = r_obs, .width = 0.9) |> \n    left_join(Kline, by = \"obs\") |> \n    ggplot(aes(x = estimate, y = log(total_tools), xmin = .lower, xmax = .upper)) +\n    geom_pointinterval() + \n    theme_classic(base_size = 15) + \n    labs(x = expression(nu), y = \"log(total tools)\", title = \"y ~ Fitted OLRE\")\n```\n\n::: {.cell-output-display}\n![Fitted nu parameters as a function of the natural log of the response, total tools. Bars represent 90% credible intervals.](index_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n:::\n\n\n\n## PyMC\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstats = az.summary(idata_OLRE, var_names=[\"^nu.*\"], filter_vars=\"regex\", hdi_prob=0.9)\n\nfig = plt.figure(figsize=(20, 13))\nplt.hlines(y=idata_OLRE.observed_data['y'], xmin=stats['hdi_5%'], xmax = stats['hdi_95%'], color=\"black\")\nplt.scatter(y=idata_OLRE.observed_data['y'], x=stats['mean'], color=\"black\")\n\nplt.title('y ~ Fitted OLRE', loc = \"left\")\nplt.xlabel(r'$\\nu$')\nplt.ylabel('log(total tools)')\n```\n\n::: {.cell-output-display}\n![Fitted nu parameters as a function of the natural log of the response, total tools. Bars represent 90% credible intervals.](index_files/figure-html/unnamed-chunk-7-1.png){width=1920}\n:::\n:::\n\n\n\n:::\n\nThis plot shows us that the OLREs are positively correlated with the values of the observed data. Why? These parameters are doing exactly what they are supposed to do: capture excess dispersion in the data by learning which points are higher or lower than we would expect, conditional on population size. But if you think for a moment about *out-of-sample* prediction, you might intuit the problem. We need to generate $y_\\text{rep}$ for a new observation with some unknown $y_{\\text{test}}$, whose value will not be known ahead of time. Thus, the OLRE should convey no information about the value $y_{\\text{test}}$. In our naive predictive check, we have mistakenly treated $\\nu$ as fixed, when really it should be replicated along with $y_\\text{rep}$, akin to $\\epsilon$ in a linear regression. Generating $y_\\text{rep}$ this way is referred to as \"mixed replication\", because we leave the hyperparameter $\\sigma$ fixed but replicate each random effect parameter [@gelman1996posterior].\n\n# The right way\n\nThe way out of this is straightforward. All we have to do is replace the fitted OLREs with new levels, denoted $\\nu_{\\text{rep}}$, which are generated using posterior draws of the observation-level standard deviation $\\sigma$. \n\n:::{.panel-tabset}\n\n## brms\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyrep_OLRE <- m_poisson_OLRE |> \n    posterior_predict(newdata = Kline |> \n    mutate(obs = paste(\"OLRE_rep\", 1:n())),\n     allow_new_levels = TRUE,\n     sample_new_levels = \"gaussian\")\n\nbayesplot::ppc_dens_overlay(Kline$total_tools, yrep_OLRE[1:100,]) + theme(legend.position = \"none\") + bayesplot::ppc_intervals(Kline$total_tools, yrep_OLRE) +\n    plot_annotation(subtitle = \"OLRE PPC: The right way\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n:::\n\n\n\n## PyMC\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport xarray as xr\n\nwith m_poisson_OLRE:\n    nu_rep = pm.Normal(\"nu_rep\", mu = 0, sigma = sigma, shape = len(Kline))\n    log_lam = b0 + nu_rep + b_pop * Kline['log_pop_z']\n    yrep = pm.Poisson(\"yrep\", pm.math.exp(log_lam))\n\npred_yrep = pm.sample_posterior_predictive(idata_OLRE, m_poisson_OLRE, predictions=True, extend_inferencedata=False, var_names = ['nu_rep', 'yrep'], random_seed=2)\n```\n\n```{.python .cell-code}\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 13))\n\nplot_ppc_dens(idata.observed_data['y'], pred_yrep.predictions['yrep'], axes, 0)\nplot_ppc_intervals(idata.observed_data['y'], pred_yrep.predictions['yrep'], axes, 1)\naxes[0].set_title('OLRE PPC: The right way', loc = \"left\")\n```\n\n::: {.cell-output-display}\n![Correct posterior predictive checks for OLRE Poisson model, sampling new levels of nu (nu_rep). (left) replicated and observed densities, (right) observation-level reps, with bars representing 50% and 90% credible intervals.](index_files/figure-html/unnamed-chunk-9-1.png){width=1920}\n:::\n:::\n\n\n\n:::\n\nNotice that, unlike our first predictive check with no OLRE, the credible intervals of $y_{\\text{rep}}$ all contain the observed values of y. But unlike our (wrong) second predictive check, the predictions do not conform so closely to the observed values, because each $\\nu_{\\text{rep}}$ is independent of $y$. This provides us with a more realistic picture of our model's fit. So, why do most posterior predictive functions treat OLREs the wrong way by default? The answer is mundane: the software doesn't know whether $\\nu$ is an OLRE or instead a parameter that should be fixed across replications, like random effects for group differences. The latter is more common, so the default is sensible--but not necessarily safe.\n\nThere is a caveat: all posterior predictive checks are overly optimistic for *out-of-sample* data, because we are using the same data points to both fit and evaluate the model. As a result, a more complex model that looks better in a predictive check might actually look worse when generalizing to new data, i.e., overfitting. This issue is not specific to OLREs, but in the final section I'll show you how to address overfitting in predictive checks.\n\n\n# An even better way?\n\nUsing leave-one-out cross validation, (LOOCV) we can construct predictions where one observation at a time is left out of model fitting and then used as a test point. This offers a more honest appraisal of predictive accuracy, because the model does not have access to the left out $y$ before we try to predict them. Now we have a true prediction task, rather than retrodicting the sample data.\n\n:::{.panel-tabset}\n\n## brms\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyrep_loo <- matrix(NA, nrow = ndraws(m_poisson_OLRE), ncol = nrow(Kline))\n\nfor (i in 1:nrow(Kline)) {\n    model_loo <- update(m_poisson_OLRE, newdata = Kline[-i,], seed = 123)\n    yrep_loo[,i] <- posterior_predict(model_loo, newdata = Kline[i,], allow_new_levels = TRUE, sample_new_levels = \"gaussian\")\n}\n\nbayesplot::ppc_dens_overlay(Kline$total_tools, yrep_loo[1:100,]) + theme(legend.position = \"none\") + bayesplot::ppc_intervals(Kline$total_tools, yrep_loo) +\n    plot_annotation(subtitle = \"OLRE PPC-LOO\") \n```\n\n::: {.cell-output-display}\n![LOOCV posterior predictive checks for OLRE Poisson model. Observation-level reps, with bars representing 50% and 90% credible intervals.](index_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n:::\n\n\n\n## PyMC\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef model_factory(train, test):\n    obs_idx = np.arange(len(train))\n    coords = {\"obs\": obs_idx}\n\n    with pm.Model(coords = coords) as model:\n        b0 = pm.Normal(\"Intercept\", mu=3, sigma=0.5)\n        b_pop = pm.Normal(\"slope\", mu=0, sigma=0.2)\n        nu = pm.Normal(\"nu\", mu = 0, sigma = 1, dims = \"obs\")\n        sigma = pm.Exponential(\"sigma\", lam = 2)\n        # linear model\n        log_lam = b0 + nu[obs_idx]*sigma + b_pop * train['log_pop_z'] \n        ## Poisson likelihood\n        y = pm.Poisson(\"y\", mu=pm.math.exp(log_lam), observed=train['total_tools'])\n\n        nu_rep = pm.Normal(\"nu_rep\", mu = 0, sigma = sigma)\n        yrep = pm.Poisson(\"yrep\", pm.math.exp(b0 + nu_rep + b_pop * test['log_pop_z']))\n    \n        idata_loo = pm.sample(1000, chains = 1, target_accept = 0.99, random_seed=2)\n    \n    pred = pm.sample_posterior_predictive(idata_loo, model, predictions=True, extend_inferencedata=False, var_names = ['yrep'], random_seed=2)\n\n    return pred.predictions['yrep']\n\ntest_preds = []\n\nfor i in range(len(Kline)):\n    train = Kline.drop(index = Kline.index[i])\n    test = Kline.iloc[[i]]\n    yrep_loo = model_factory(train, test)\n    test_preds.append(yrep_loo)\n```\n\n```{.python .cell-code}\n\nloo_preds_combined = xr.concat(test_preds, dim='obs').transpose('chain', 'draw', 'obs', 'yrep_dim_2').squeeze('yrep_dim_2')\n        \nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 13))\nplot_ppc_dens(idata.observed_data['y'], loo_preds_combined, axes, 0)\nplot_ppc_intervals(idata.observed_data['y'], loo_preds_combined, axes, 1)\naxes[0].set_title('OLRE PPC-LOO', loc = \"left\")\n```\n\n::: {.cell-output-display}\n![Exact LOOCV posterior predictive checks for OLRE Poisson model. Observation-level reps, with bars representing 50% and 90% credible intervals.](index_files/figure-html/unnamed-chunk-11-1.png){width=1920}\n:::\n:::\n\n\n\n:::\n\nThis predictive check is even less optimistic, but it still looks better than the basic Poisson model we fit, suggesting that the OLRE is helpful. For larger models, it would be infeasible to refit $N$ times, so one might turn to Pareo-smoothed importance sampling as an approximation (PSIS-LOO) [@vehtari2017practical]. However, my understanding is that PSIS is not reliable for these types of models, and one should instead [integrate out the OLREs using adaptive quadrature](https://users.aalto.fi/~ave/modelselection/roaches.html#5_Poisson_model_with_varying_intercept_and_integrated_LOO).\n\n# Reproducible environment\n\n[R session info](R_session_info.txt)\n\n[Conda environment](conda_environment.txt)\n\n# References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}